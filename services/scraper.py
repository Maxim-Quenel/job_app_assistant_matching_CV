import time
import pandas as pd
import urllib.parse
import re
import os
from selenium import webdriver
from selenium.webdriver.edge.options import Options
from selenium.webdriver.common.by import By

# Ensure data directory exists
DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data')
if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

# --- 1. FONCTION DE NETTOYAGE ---
def clean_text(text):
    if not isinstance(text, str):
        return str(text) if text is not None else ""
    text = text.replace('\n', ' ').replace('\r', ' ')
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

# --- 2. FONCTION POUR SÃ‰PARER LE TEXTE ---
def extract_mission_profil(full_text):
    if not full_text:
        return "", ""

    # 1. Nettoyage du "Header" (Navigation, Login, etc.)
    # On cherche un marqueur fort de dÃ©but de description
    start_markers = ["les missions du poste", "le poste", "description du poste", "Ã  propos du poste", "vos missions"]
    text_lower = full_text.lower()
    
    start_index = 0
    for marker in start_markers:
        idx = text_lower.find(marker)
        if idx != -1:
            # On prend le premier marqueur trouvÃ© comme dÃ©but rÃ©el
            start_index = idx
            break
            
    # Si on a trouvÃ© un marqueur, on coupe tout ce qu'il y a avant
    if start_index > 0:
        relevant_text = full_text[start_index:]
    else:
        relevant_text = full_text

    text_lower = relevant_text.lower()
    
    # 2. SÃ©paration Mission / Profil
    profil_keywords = ["le profil recherchÃ©", "ce que nous recherchons", "votre profil", "profil attendu", "profil"]
    
    split_index = -1
    for keyword in profil_keywords:
        idx = text_lower.find(keyword)
        # On s'assure que le mot clÃ© n'est pas au tout dÃ©but (titre)
        if idx != -1 and idx > 10: 
            split_index = idx
            break
    
    if split_index != -1:
        missions = relevant_text[:split_index]
        profil = relevant_text[split_index:]
    else:
        # Fallback si pas de sÃ©paration nette
        if len(relevant_text) > 500:
            missions = relevant_text
            profil = "Non sÃ©parÃ© automatiquement (voir colonne Missions)"
        else:
            missions = relevant_text
            profil = ""
        
    return clean_text(missions), clean_text(profil)

# --- 3. GESTION DES COOKIES ---
def handle_cookies(driver):
    try:
        time.sleep(1)
        # Tente de trouver le bouton "Continuer sans accepter" ou "Refuser"
        buttons = driver.find_elements(By.XPATH, "//button[contains(text(), 'Continuer sans') or contains(text(), 'Refuser') or contains(@id, 'close')]")
        if buttons:
            driver.execute_script("arguments[0].click();", buttons[0])
    except:
        pass

# --- 4. DÃ‰PLIER LE PROFIL (CRUCIAL) ---
def expand_profil_section(driver):
    """
    Cherche le bouton/header 'Profil recherchÃ©' et force le clic via JS.
    """
    try:
        # On cherche un Ã©lÃ©ment visible contenant "Profil"
        targets = driver.find_elements(By.XPATH, "//*[contains(text(), 'Profil') or contains(text(), 'profil')]")
        
        for target in targets:
            # On clique uniquement si c'est un titre pertinent (h2, h3, button, span dans un header)
            tag_name = target.tag_name.lower()
            text_cont = target.text.lower()
            
            if ("recherchÃ©" in text_cont or "attendu" in text_cont) and tag_name in ['h2', 'h3', 'h4', 'span', 'button', 'div']:
                driver.execute_script("arguments[0].click();", target)
                time.sleep(0.5) # Laisser le temps Ã  l'animation
                break # Un seul clic suffit gÃ©nÃ©ralement
    except Exception:
        pass

# --- 5. FONCTION PRINCIPALE ---
def scrape_jobs(keyword, num_jobs=10, progress_callback=None):
    if progress_callback:
        progress_callback(f"ðŸš€ DÃ©marrage de la recherche pour : {keyword}")
    
    # Check if keyword is a URL
    if keyword.strip().startswith('http'):
        url = keyword.strip()
        # Decode url to try to guess a proper "keyword" for logging (optional)
        try:
             # simple heuristic to find 'k=' param or just use full url
             parsed = urllib.parse.urlparse(url)
             qs = urllib.parse.parse_qs(parsed.query)
             if 'k' in qs:
                 progress_callback(f"ðŸ”— URL dÃ©tectÃ©e. Mot-clÃ© extrait : {qs['k'][0]}")
             else:
                 progress_callback(f"ðŸ”— URL directe dÃ©tectÃ©e.")
        except:
             pass
    else:
        encoded_keyword = urllib.parse.quote_plus(keyword)
        url = f"https://www.hellowork.com/fr-fr/emploi/recherche.html?k={encoded_keyword}"
    
    options = Options()
    options.add_argument("--start-maximized")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    options.add_argument("--disable-blink-features=AutomationControlled")
    
    # Edge Driver Path (Root of workspace)
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    driver_path = os.path.join(project_root, "msedgedriver.exe")
    
    service = webdriver.edge.service.Service(driver_path) if os.path.exists(driver_path) else None
    
    # Init driver
    if service:
        driver = webdriver.Edge(options=options, service=service)
    else:
        driver = webdriver.Edge(options=options)

    all_jobs_data = []
    
    try:
        # --- PHASE 1 : LINK & LOCATION SCRAPING ---
        if progress_callback:
            progress_callback("1ï¸âƒ£  RÃ©cupÃ©ration des liens et lieux...")
        
        driver.get(url)
        time.sleep(3)
        handle_cookies(driver)
        
        job_links = []
        
        # --- DETECTION: Is this a Single Job Page? ---
        # Hueristic: URL contains /emplois/ and .html, or we find a "Postuler" button immediately
        is_single_job = False
        if "/emplois/" in url and ".html" in url:
             is_single_job = True
        
        if is_single_job:
            try:
                # Extract basic info from the opened page
                # These selectors are approximative and might need adjustment based on HelloWork's specific DOM for job pages
                
                # Title often in h1
                raw_title = driver.find_element(By.TAG_NAME, "h1").text
                
                # Company often in a span or div near title, or we can just leave it blank and let Phase 2 refine it?
                # Actually Phase 2 re-visits the link, so we just need a valid 'link' entry.
                # But Phase 2 expects 'Poste', 'Entreprise', 'Lieu' to log progress.
                
                raw_company = "Voir dÃ©tail"
                raw_location = "Voir dÃ©tail"
                
                # Attempt to find company/location from metadata if easier
                # But to be safe and simple:
                job_links.append({
                    "Poste": clean_text(raw_title),
                    "Entreprise": raw_company,
                    "Lieu": raw_location,
                    "Lien": url
                })
                
                if progress_callback:
                    progress_callback("âœ… URL offre unique dÃ©tectÃ©e.")
                    
            except Exception as e:
                # Fallback if detection failed or selectors changed
                 if progress_callback:
                    progress_callback(f"âš ï¸ Tentative lecture offre unique Ã©chouÃ©e: {e}")
        else:
            # --- STANDARD SEARCH RESULTS SCRAPING ---
            potential_jobs = driver.find_elements(By.CSS_SELECTOR, "ul > li")
            
            for card in potential_jobs:
                if len(job_links) >= num_jobs: break
                try:
                    # Check for h3 title
                    if card.find_elements(By.TAG_NAME, "h3"):
                        link_elem = card.find_element(By.TAG_NAME, "a")
                        link = link_elem.get_attribute("href")
                        
                        # Raw text extraction
                        raw_title = card.find_element(By.TAG_NAME, "h3").text.split('\n')[0]
                        txt_lines = card.text.split('\n')
                        
                        # Heuristique pour l'entreprise et le lieu
                        raw_company = txt_lines[1] if len(txt_lines) > 1 else "N/A"
                        
                        # RÃ‰CUPÃ‰RATION DU LIEU ICI
                        raw_location = "N/A"
                        if len(txt_lines) > 2:
                            potential_loc = txt_lines[2]
                            if len(potential_loc) < 50:
                                raw_location = potential_loc
                        
                        job_links.append({
                            "Poste": clean_text(raw_title),
                            "Entreprise": clean_text(raw_company),
                            "Lieu": clean_text(raw_location),
                            "Lien": link
                        })
                except Exception as e:
                    continue
            
        if progress_callback:
            progress_callback(f"âœ… {len(job_links)} offres trouvÃ©es. Analyse dÃ©taillÃ©e...")
        
        # --- PHASE 2 : DETAILED SCRAPING ---
        for index, job in enumerate(job_links):
            msg = f"Traitement {index + 1}/{len(job_links)} : {job['Poste']} - {job['Lieu']}"
            if progress_callback:
                progress_callback(msg)
            else:
                print(msg)
            
            try:
                driver.get(job['Lien'])
                time.sleep(2) 
                handle_cookies(driver)

                # Clic pour ouvrir le profil (Improved)
                expand_profil_section(driver)
                
                # RÃ©cupÃ©ration du texte (Improved)
                try:
                    full_desc_elem = driver.find_element(By.TAG_NAME, "main")
                    full_desc = full_desc_elem.text
                except:
                    full_desc = driver.find_element(By.TAG_NAME, "body").text

                missions, profil = extract_mission_profil(full_desc)
                
                # Rescue Fallback
                if len(profil) < 20:
                    try:
                        xpath_rescue = "//*[contains(text(), 'Profil')]/following-sibling::div"
                        profil_elem = driver.find_element(By.XPATH, xpath_rescue)
                        profil = clean_text(profil_elem.text)
                    except:
                        pass

                all_jobs_data.append({
                    "Poste": job['Poste'],
                    "Entreprise": job['Entreprise'],
                    "Lieu": job['Lieu'],
                    "Missions": missions,
                    "Profil_Recherche": profil,
                    "Lien": job['Lien']
                })
                
            except Exception as e:
                if progress_callback:
                    progress_callback(f"âŒ Erreur sur {job['Poste']} : {e}")
                job_fallback = job.copy()
                job_fallback["Missions"] = "Erreur accÃ¨s"
                job_fallback["Profil_Recherche"] = ""
                all_jobs_data.append(job_fallback)

    except Exception as e:
        if progress_callback:
            progress_callback(f"âŒ Erreur critique : {e}")
        
    finally:
        driver.quit()
        if progress_callback:
            progress_callback("âœ… Scraping terminÃ©.")

    df = pd.DataFrame(all_jobs_data)
    output_path = os.path.join(DATA_DIR, "jobs_raw.csv")
    df.to_csv(output_path, index=False, encoding='utf-8-sig', sep=',')
    
    return output_path
