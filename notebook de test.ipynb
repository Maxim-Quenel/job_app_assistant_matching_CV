{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33ed40f4",
   "metadata": {},
   "source": [
    "# R√©cup√®re 5 offres d'emplois sur hellowork √† partir d'un mot cl√© (ici : Data Analyst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5015e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import re  # Import n√©cessaire pour le nettoyage des espaces\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# --- 1. FONCTION DE NETTOYAGE (NOUVEAU) ---\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Nettoie le texte pour garantir qu'il tient sur une seule ligne.\n",
    "    - Remplace les sauts de ligne (\\n, \\r) par des espaces.\n",
    "    - Supprime les espaces multiples inutiles.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return str(text) if text is not None else \"\"\n",
    "    \n",
    "    # Remplace les retours √† la ligne par un espace simple\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    \n",
    "    # Remplace les suites d'espaces (ex: \"  \") par un seul espace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# --- 2. FONCTION POUR S√âPARER LE TEXTE ---\n",
    "def extract_mission_profil(full_text):\n",
    "    \"\"\"\n",
    "    Essaie de couper le texte en deux parties : Missions et Profil.\n",
    "    Renvoie les textes nettoy√©s (sans sauts de ligne).\n",
    "    \"\"\"\n",
    "    if not full_text:\n",
    "        return \"\", \"\"\n",
    "\n",
    "    text_lower = full_text.lower()\n",
    "    \n",
    "    profil_keywords = [\"profil\", \"ce que nous recherchons\", \"votre profil\", \"comp√©tences\", \"pr√©-requis\", \"attendus\"]\n",
    "    \n",
    "    split_index = -1\n",
    "    for keyword in profil_keywords:\n",
    "        idx = text_lower.find(keyword)\n",
    "        if idx != -1:\n",
    "            # On √©vite de couper si le mot cl√© est au tout d√©but (ex: titre)\n",
    "            if idx > len(full_text) * 0.1: \n",
    "                split_index = idx\n",
    "                break\n",
    "    \n",
    "    if split_index != -1:\n",
    "        missions = full_text[:split_index]\n",
    "        profil = full_text[split_index:]\n",
    "    else:\n",
    "        missions = full_text\n",
    "        profil = \"Non identifi√© sp√©cifiquement (voir colonne Missions)\"\n",
    "        \n",
    "    # IMPORTANT : On nettoie le texte avant de le renvoyer\n",
    "    return clean_text(missions), clean_text(profil)\n",
    "\n",
    "# --- 3. FONCTION PRINCIPALE ---\n",
    "def get_jobs_detailed_hellowork(keyword, num_jobs=10):\n",
    "    print(f\"üöÄ D√©marrage de la recherche approfondie pour : {keyword}...\")\n",
    "    \n",
    "    encoded_keyword = urllib.parse.quote_plus(keyword)\n",
    "    url = f\"https://www.hellowork.com/fr-fr/emploi/recherche.html?k={encoded_keyword}\"\n",
    "    \n",
    "    options = Options()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    # User agent pour √©viter d'√™tre bloqu√© trop vite\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    \n",
    "    driver = webdriver.Edge(options=options)\n",
    "    all_jobs_data = []\n",
    "    \n",
    "    try:\n",
    "        # --- PHASE 1 : R√âCUP√âRATION DES LIENS ET DU LIEU ---\n",
    "        print(\"1Ô∏è‚É£  PHASE 1 : R√©cup√©ration des liens et des lieux...\")\n",
    "        driver.get(url)\n",
    "        time.sleep(4)\n",
    "        \n",
    "        # Gestion des cookies\n",
    "        try:\n",
    "            buttons = driver.find_elements(By.TAG_NAME, \"button\")\n",
    "            for btn in buttons:\n",
    "                if \"continuer sans\" in btn.text.lower() or \"refuser\" in btn.text.lower():\n",
    "                    btn.click()\n",
    "                    break\n",
    "            time.sleep(1)\n",
    "        except: pass\n",
    "\n",
    "        potential_jobs = driver.find_elements(By.CSS_SELECTOR, \"ul > li\")\n",
    "        job_links = []\n",
    "        \n",
    "        for card in potential_jobs:\n",
    "            if len(job_links) >= num_jobs:\n",
    "                break\n",
    "            try:\n",
    "                if card.find_elements(By.TAG_NAME, \"h3\"):\n",
    "                    link_elem = card.find_element(By.TAG_NAME, \"a\")\n",
    "                    link = link_elem.get_attribute(\"href\")\n",
    "                    \n",
    "                    # Titre\n",
    "                    raw_title = card.find_element(By.TAG_NAME, \"h3\").text.split('\\n')[0]\n",
    "                    \n",
    "                    # Extraction Entreprise et Lieu via le texte brut de la carte\n",
    "                    txt_lines = card.text.split('\\n')\n",
    "                    \n",
    "                    # L'entreprise est souvent en 2√®me ligne\n",
    "                    raw_company = txt_lines[1] if len(txt_lines) > 1 else \"N/A\"\n",
    "                    if raw_company == raw_title and len(txt_lines) > 2: \n",
    "                         raw_company = txt_lines[2]\n",
    "\n",
    "                    # Le lieu est souvent en 3√®me ligne\n",
    "                    raw_location = \"N/A\"\n",
    "                    if len(txt_lines) > 2:\n",
    "                        potential_loc = txt_lines[2]\n",
    "                        if len(potential_loc) < 50: \n",
    "                            raw_location = potential_loc\n",
    "                    \n",
    "                    # On stocke les versions nettoy√©es\n",
    "                    job_links.append({\n",
    "                        \"Poste\": clean_text(raw_title),\n",
    "                        \"Entreprise\": clean_text(raw_company),\n",
    "                        \"Lieu\": clean_text(raw_location),\n",
    "                        \"Lien\": link\n",
    "                    })\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "        print(f\"‚úÖ {len(job_links)} liens trouv√©s. Passage √† l'extraction d√©taill√©e.\")\n",
    "        \n",
    "        # --- PHASE 2 : VISITE DE CHAQUE PAGE ---\n",
    "        print(\"2Ô∏è‚É£  PHASE 2 : Analyse d√©taill√©e des annonces (Missions/Profil)...\")\n",
    "        \n",
    "        for index, job in enumerate(job_links):\n",
    "            print(f\"   ‚è≥ Traitement {index + 1}/{len(job_links)} : {job['Poste']}...\")\n",
    "            \n",
    "            try:\n",
    "                driver.get(job['Lien'])\n",
    "                time.sleep(3) \n",
    "                \n",
    "                full_desc = \"\"\n",
    "                try:\n",
    "                    sections = driver.find_elements(By.TAG_NAME, \"section\")\n",
    "                    # On prend la section avec le plus de texte, souvent la description\n",
    "                    longest_section = max(sections, key=lambda x: len(x.text) if x.text else 0)\n",
    "                    full_desc = longest_section.text\n",
    "                except:\n",
    "                    full_desc = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "\n",
    "                # Extraction et nettoyage via la fonction modifi√©e\n",
    "                missions, profil = extract_mission_profil(full_desc)\n",
    "                \n",
    "                all_jobs_data.append({\n",
    "                    \"Poste\": job['Poste'],\n",
    "                    \"Entreprise\": job['Entreprise'],\n",
    "                    \"Lieu\": job['Lieu'],\n",
    "                    \"Missions\": missions,  # D√©j√† nettoy√© par extract_mission_profil\n",
    "                    \"Profil_Recherche\": profil, # D√©j√† nettoy√© par extract_mission_profil\n",
    "                    \"Lien\": job['Lien']\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Erreur sur ce lien : {e}\")\n",
    "                job_fallback = job.copy()\n",
    "                job_fallback[\"Missions\"] = \"Erreur extraction\"\n",
    "                job_fallback[\"Profil_Recherche\"] = \"Erreur extraction\"\n",
    "                all_jobs_data.append(job_fallback)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur critique : {e}\")\n",
    "        \n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"‚úÖ Termin√©.\")\n",
    "\n",
    "    return pd.DataFrame(all_jobs_data)\n",
    "\n",
    "# --- EXECUTION ET SAUVEGARDE ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Lancer le scraping\n",
    "    df_details = get_jobs_detailed_hellowork(\"Data Analyst\", 5)\n",
    "\n",
    "    # Affichage de contr√¥le\n",
    "    pd.set_option('display.max_colwidth', 50)\n",
    "    print(\"\\nAper√ßu des donn√©es :\")\n",
    "    print(df_details[[\"Poste\", \"Lieu\", \"Missions\"]].head())\n",
    "\n",
    "    # Export CSV \"PROPRE\"\n",
    "    nom_fichier = \"hellowork_detailed_jobs_clean2.csv\"\n",
    "    \n",
    "    # encoding='utf-8-sig' est important pour qu'Excel lise bien les accents\n",
    "    # index=False √©vite d'avoir une colonne de num√©ros de ligne (0, 1, 2...)\n",
    "    df_details.to_csv(nom_fichier, index=False, encoding='utf-8-sig', sep=',')\n",
    "    \n",
    "    print(f\"\\nüìÅ Fichier CSV propre g√©n√©r√© : {nom_fichier}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b0e3fd",
   "metadata": {},
   "source": [
    "# Reformule les annonce grace √† Qwen 2.5 1.5B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb69a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "\n",
    "# 1. Configuration du mod√®le\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "print(f\"Chargement du mod√®le {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 2. Chargement des donn√©es\n",
    "try:\n",
    "    df = pd.read_csv('hellowork_detailed_jobs_clean2.csv')\n",
    "    print(f\"CSV charg√© : {len(df)} offres trouv√©es.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Erreur : Fichier CSV non trouv√©.\")\n",
    "    exit()\n",
    "\n",
    "# Liste pour stocker les r√©sultats au fur et √† mesure\n",
    "resumes_stockes = []\n",
    "\n",
    "# 3. Boucle avec Streaming\n",
    "print(\"\\n=== D√âBUT DE LA G√âN√âRATION EN DIRECT ===\\n\")\n",
    "\n",
    "# On utilise un iterrows() pour traiter ligne par ligne\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    # Affichage visuel pour s√©parer les offres dans le terminal\n",
    "    print(f\"\\n\\n--- Traitement de l'offre {index + 1}/{len(df)} : {row['Poste']} chez {row['Entreprise']} ---\")\n",
    "    print(\"R√âPONSE EN DIRECT : \", end=\"\", flush=True)\n",
    "\n",
    "    prompt = f\"\"\"Tu es un expert en recrutement. Analyse l'offre d'emploi ci-dessous et g√©n√®re un r√©sum√© structur√©.\n",
    "    \n",
    "    Donn√©es de l'offre :\n",
    "    - Poste : {row['Poste']}\n",
    "    - Entreprise : {row['Entreprise']}\n",
    "    - Lieu : {row['Lieu']}\n",
    "    - Missions : {row['Missions']}\n",
    "    - Profil Recherch√© : {row['Profil_Recherche']}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    G√©n√®re la r√©ponse uniquement sous ce format strict :\n",
    "    RESUME_MATCHING:\n",
    "    - Type de profil recherch√©\n",
    "    - comp√©tences cl√©s: [Liste]\n",
    "    - Soft_Skills: [Liste]\n",
    "    - Seniority: [Niveau]\n",
    "    - Core_Mission: [Phrase r√©sum√©e]\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Tu es un assistant utile.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # C'est ici que la magie op√®re : Le Streamer\n",
    "    # skip_prompt=True permet de ne pas r√©-afficher le prompt, juste la r√©ponse de l'IA\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=500,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        streamer=streamer # <--- On active l'affichage en direct ici\n",
    "    )\n",
    "    \n",
    "    # On doit quand m√™me d√©coder le r√©sultat pour le sauvegarder dans le CSV\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # Nettoyage et stockage dans la liste\n",
    "    clean_response = response_text.replace('RESUME_MATCHING:', '').strip()\n",
    "\n",
    "    # --- MODIFICATION ICI ---\n",
    "    # Remplacement des sauts de ligne (\\n) par un s√©parateur unique (\" | \")\n",
    "    # Cela permet d'avoir tout le r√©sum√© sur une seule ligne physique dans le CSV\n",
    "    clean_response_oneline = clean_response.replace('\\n', ' | ').replace('\\r', '')\n",
    "\n",
    "    resumes_stockes.append(clean_response_oneline)\n",
    "\n",
    "# 4. Sauvegarde finale\n",
    "df['Resume_IA'] = resumes_stockes\n",
    "df.to_csv('offres_resumees_qwen_live.csv', index=False)\n",
    "print(\"\\n\\n=== TERMIN√â : Fichier sauvegard√© ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f80276",
   "metadata": {},
   "source": [
    "# Transforme un CV pdf en .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01861ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Remplace par le chemin exact de ton fichier\n",
    "CHEMIN_PDF = \"cv Qu√©nel.pdf\" \n",
    "# ---------------------\n",
    "\n",
    "def nettoyer_texte_avance(texte: str) -> str:\n",
    "    \"\"\"\n",
    "    Nettoie les artefacts courants d'extraction PDF :\n",
    "    - Recolle les lettres espac√©es (ex: \"L a n g u e s\" -> \"Langues\")\n",
    "    - Supprime les sauts de ligne excessifs\n",
    "    - Normalise les espaces\n",
    "    \"\"\"\n",
    "    if not texte:\n",
    "        return \"\"\n",
    "\n",
    "    # 1. Correction du \"Kerning\" (Lettres espac√©es : L a n g u e s ou 0 6 1 1...)\n",
    "    # Cette regex cherche une s√©quence de lettres/chiffres isol√©s s√©par√©s par un espace\n",
    "    def replacer_lettres_isolees(match):\n",
    "        return match.group(0).replace(\" \", \"\")\n",
    "    \n",
    "    # Motif : (Caract√®re + Espace) r√©p√©t√© au moins 3 fois, suivi d'un caract√®re\n",
    "    texte = re.sub(r'(?:\\b[A-Za-z0-9√Ä-√ø]\\s){3,}[A-Za-z0-9√Ä-√ø]\\b', replacer_lettres_isolees, texte)\n",
    "\n",
    "    # 2. Remplacer les sauts de ligne multiples par un seul saut\n",
    "    texte = re.sub(r'\\n\\s*\\n', '\\n\\n', texte)\n",
    "\n",
    "    # 3. Remplacer les espaces multiples (horizontaux) par un seul espace\n",
    "    texte = re.sub(r'[ \\t]+', ' ', texte)\n",
    "\n",
    "    return texte.strip()\n",
    "\n",
    "def extraire_cv_pro(chemin_fichier: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extrait le texte d'un CV en respectant au mieux la mise en page (colonnes).\n",
    "    Utilise pdfplumber pour une pr√©cision professionnelle.\n",
    "    \"\"\"\n",
    "    # V√©rification de l'existence du fichier\n",
    "    if not os.path.exists(chemin_fichier):\n",
    "        print(f\"‚ùå Erreur : Le fichier '{chemin_fichier}' est introuvable.\")\n",
    "        return None\n",
    "\n",
    "    texte_global = \"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"üîÑ Traitement de '{chemin_fichier}' avec pdfplumber...\")\n",
    "        \n",
    "        with pdfplumber.open(chemin_fichier) as pdf:\n",
    "            if not pdf.pages:\n",
    "                print(\"‚ö†Ô∏è Le PDF semble vide.\")\n",
    "                return None\n",
    "\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                # extract_text() de pdfplumber g√®re mieux les colonnes que pypdf\n",
    "                texte_page = page.extract_text(x_tolerance=2, y_tolerance=2)\n",
    "                \n",
    "                if texte_page:\n",
    "                    texte_global += f\"\\n--- PAGE {i+1} ---\\n\"\n",
    "                    texte_global += nettoyer_texte_avance(texte_page)\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Page {i+1} : Aucun texte s√©lectionnable (image scan ?).\")\n",
    "\n",
    "        print(\"‚úÖ Extraction termin√©e avec succ√®s.\")\n",
    "        return texte_global\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Une erreur critique est survenue : {e}\")\n",
    "        return None\n",
    "\n",
    "# --- EX√âCUTION ---\n",
    "\n",
    "# Appel de la fonction\n",
    "contenu_cv = extraire_cv_pro(CHEMIN_PDF)\n",
    "\n",
    "# Affichage du r√©sultat si l'extraction a fonctionn√©\n",
    "if contenu_cv:\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"      APER√áU DU CONTENU EXTRAIT\")\n",
    "    print(\"=\"*40 + \"\\n\")\n",
    "    print(contenu_cv)\n",
    "    \n",
    "    # Optionnel : Sauvegarde dans un fichier texte pour v√©rification\n",
    "    with open(\"resultat_cv_quenel.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(contenu_cv)\n",
    "    print(\"\\nüíæ Le r√©sultat a √©galement √©t√© sauvegard√© dans 'resultat_cv_celyann.txt'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09d4f85",
   "metadata": {},
   "source": [
    "# Reformule le cv avec Qwen 2.5 1.5B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e767cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "import os\n",
    "\n",
    "# Configuration du mod√®le\n",
    "MODEL_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"D√©termine le meilleur p√©riph√©rique disponible (CUDA, MPS ou CPU).\"\"\"\n",
    "    if torch.cuda.is_available(): return \"cuda\"\n",
    "    if torch.backends.mps.is_available(): return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "def synthesize_cv_for_matching(file_path):\n",
    "    # 1. Lecture du fichier\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Erreur : Le fichier '{file_path}' est introuvable.\")\n",
    "        return None\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        cv_content = f.read()\n",
    "\n",
    "    print(\"Chargement du mod√®le en cours... (cela peut prendre quelques secondes)\")\n",
    "    \n",
    "    # 2. Chargement du mod√®le et du tokenizer\n",
    "    device = get_device()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map=device)\n",
    "\n",
    "    # Streamer pour l'affichage en direct\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    # 3. LE PROMPT \"SYNTH√àSE & MATCHING\"\n",
    "    system_prompt = \"\"\"Tu es un assistant de synth√®se RH.\n",
    "    Ta fonction est de transformer les informations professionnelles d'un CV en une fiche de comp√©tences standardis√©e.\n",
    "    Tu dois reformuler le contenu pour qu'il soit totalement neutre et g√©n√©rique.\n",
    "    Remplace syst√©matiquement l'identit√© du candidat par le terme : \"Candidat\".\n",
    "    Concentre-toi uniquement sur les savoir-faire, les dipl√¥mes et l'exp√©rience m√©tier.\"\"\"\n",
    "\n",
    "    # ON INVERSE : Le CV est maintenant tout en haut\n",
    "    user_prompt = f\"\"\"\n",
    "    DOCUMENT √Ä ANALYSER :\n",
    "    ---\n",
    "    {cv_content}\n",
    "    ---\n",
    "\n",
    "    INSTRUCTIONS :\n",
    "    A partir du texte ci-dessus, extrais et classe les informations professionnelles.\n",
    "    Remplis STRICTEMENT le mod√®le ci-dessous.\n",
    "    Ne r√©p√®te pas le texte original. Arr√™te-toi apr√®s la section 5.\n",
    "\n",
    "    MODELE √Ä REMPLIR :\n",
    "\n",
    "    ### 1. Synth√®se du Profil\n",
    "    **Intitul√© du poste** : [Indiquer le m√©tier principal ici]\n",
    "    **Resum√©** : Candidat exp√©riment√© dans le domaine de [Indiquer le secteur].\n",
    "\n",
    "    ### 2. Comp√©tences Techniques (Hard Skills)\n",
    "    [Lister les logiciels, outils et techniques m√©tier]\n",
    "\n",
    "    ### 3. Qualit√©s Professionnelles (Soft Skills)\n",
    "    [Lister les qualit√©s humaines et relationnelles]\n",
    "\n",
    "    ### 4. Analyse de l'Exp√©rience\n",
    "    **Niveau** : [Junior / Confirm√© / Senior]\n",
    "    **Secteurs dominants** : [Indiquer les industries]\n",
    "    **Atouts cl√©s** : [Lister les points forts professionnels]\n",
    "\n",
    "    ### 5. Formation Acad√©mique\n",
    "    [Lister uniquement les dipl√¥mes et certificats obtenus]\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    # 4. Formatage\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    print(\"\\n=== D√âBUT DE LA SYNTH√àSE (STREAMING) ===\\n\")\n",
    "\n",
    "    # 5. G√©n√©ration\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=1000,   # Une synth√®se est plus courte qu'une r√©√©criture compl√®te\n",
    "        temperature=0.2,       # Temp√©rature basse pour √™tre factuel\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1, # √âvite de r√©p√©ter les m√™mes comp√©tences\n",
    "        streamer=streamer\n",
    "    )\n",
    "\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    full_response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return full_response\n",
    "\n",
    "# ... (Tout le code pr√©c√©dent reste identique) ...\n",
    "\n",
    "def save_to_txt(content, filename):\n",
    "    \"\"\"Sauvegarde le contenu dans un fichier texte.\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(content)\n",
    "        print(f\"\\n[OK] R√©sultat sauvegard√© dans : {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERREUR] Impossible de sauvegarder le fichier : {e}\")\n",
    "\n",
    "# --- Test et Sauvegarde ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. D√©finition des fichiers\n",
    "    fichier_entree = \"resultat_cv_celyann.txt\"\n",
    "    fichier_sortie = \"synthese_finale_celyan.txt\"  # Nom du fichier de sortie\n",
    "    \n",
    "    # 2. Lancement de l'analyse\n",
    "    # Le texte s'affiche en direct gr√¢ce au Streamer\n",
    "    resultat_final = synthesize_cv_for_matching(fichier_entree)\n",
    "    \n",
    "    # 3. Sauvegarde dans le fichier .txt\n",
    "    save_to_txt(resultat_final, fichier_sortie)\n",
    "    \n",
    "    print(\"\\n=== TRAITEMENT ET SAUVEGARDE TERMIN√âS ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a3712a",
   "metadata": {},
   "source": [
    "# calcul la similarit√© cosinus √† partir d'embedding g√©n√©r√© avec bge-m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e722b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def match_cv_jobs():\n",
    "    # 1. Chargement du mod√®le demand√©\n",
    "    print(\"Chargement du mod√®le 'BAAI/bge-m3'...\")\n",
    "    model = SentenceTransformer('BAAI/bge-m3')\n",
    "\n",
    "    # 2. Lecture des fichiers\n",
    "    try:\n",
    "        # Lecture du CV\n",
    "        with open('resultat_cv_celyann.txt', 'r', encoding='utf-8') as f:\n",
    "            cv_text = f.read()\n",
    "        \n",
    "        # Lecture des annonces\n",
    "        df_jobs = pd.read_csv('offres_resumees_qwen_live.csv')\n",
    "        df_jobs = df_jobs.fillna('') # Remplacer les NaN par des cha√Ænes vides\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur de lecture des fichiers : {e}\")\n",
    "        return\n",
    "\n",
    "    # 3. Pr√©paration des donn√©es (MODIFI√â)\n",
    "    # On combine uniquement : Poste + Entreprise + Resume_IA\n",
    "    print(\"Pr√©paration des donn√©es avec les colonnes : Poste, Entreprise, Resume_IA...\")\n",
    "    \n",
    "    df_jobs['text_complet'] = (\n",
    "        df_jobs['Poste'].astype(str) + \" \" + \n",
    "        df_jobs['Entreprise'].astype(str) + \" \" + \n",
    "        df_jobs['Resume_IA'].astype(str)\n",
    "    )\n",
    "\n",
    "    print(f\"Vectorisation de {len(df_jobs)} offres en cours...\")\n",
    "\n",
    "    # 4. Vectorisation (Embeddings)\n",
    "    # Le mod√®le transforme le texte en vecteurs num√©riques\n",
    "    cv_vector = model.encode([cv_text])\n",
    "    job_vectors = model.encode(df_jobs['text_complet'].tolist())\n",
    "\n",
    "    # 5. Calcul de similarit√©\n",
    "    scores = cosine_similarity(cv_vector, job_vectors)[0]\n",
    "        \n",
    "    # Ajout du score (en pourcentage)\n",
    "    df_jobs['match_score'] = scores * 100\n",
    "    \n",
    "    # 6. Classement des r√©sultats\n",
    "    df_result = df_jobs.sort_values(by='match_score', ascending=False)\n",
    "    \n",
    "    # Affichage des meilleurs r√©sultats\n",
    "    # J'ai ajout√© 'Resume_IA' √† l'affichage pour que tu puisses v√©rifier la pertinence\n",
    "    cols_to_show = ['Poste', 'Entreprise', 'match_score', 'Resume_IA', 'Lien']\n",
    "    \n",
    "    print(\"\\n--- TOP 5 OFFRES CORRESPONDANTES ---\")\n",
    "    # On limite l'affichage de Resume_IA aux 100 premiers caract√®res pour la lisibilit√© console\n",
    "    pd.set_option('display.max_colwidth', 100) \n",
    "    print(df_result[cols_to_show].head(5).to_string(index=False))\n",
    "    \n",
    "    # Sauvegarde\n",
    "    output_filename = 'resultats_matching_resume_ia.csv'\n",
    "    df_result.to_csv(output_filename, index=False)\n",
    "    print(f\"\\nLes r√©sultats complets ont √©t√© sauvegard√©s dans '{output_filename}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    match_cv_jobs()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
